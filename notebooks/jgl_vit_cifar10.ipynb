{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeonggunlee/Vision-Transformer-Study/blob/main/Vision_Transformer_Cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MANKFlOvF5V"
   },
   "source": [
    "\n",
    "Vision-Transformers with CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-16861214/ipykernel_3820894/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LqFb9BsovDHZ"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Train CIFAR10 with PyTorch and Vision Transformers!\n",
    "written by @kentaroy47, @arutema47\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "#from models import *\n",
    "#from models.vit import ViT\n",
    "#from utils import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PyEsD1OKxm9Y",
    "outputId": "c5f996c8-114e-45dc-a8a6-9b1f9c0f5407"
   },
   "outputs": [],
   "source": [
    "# !pip install einops\n",
    "# !pip install odach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Hv-jxJqg3tl-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "'''Some helper functions for PyTorch, including:\n",
    "    - get_mean_and_std: calculate the mean and std value of dataset.\n",
    "    - msr_init: net parameter initialization.\n",
    "    - progress_bar: progress bar mimic xlua.progress.\n",
    "'''\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "def get_mean_and_std(dataset):\n",
    "    '''Compute the mean and std value of dataset.'''\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    print('==> Computing mean and std..')\n",
    "    for inputs, targets in dataloader:\n",
    "        for i in range(3):\n",
    "            mean[i] += inputs[:,i,:,:].mean()\n",
    "            std[i] += inputs[:,i,:,:].std()\n",
    "    mean.div_(len(dataset))\n",
    "    std.div_(len(dataset))\n",
    "    return mean, std\n",
    "\n",
    "def init_params(net):\n",
    "    '''Init layer parameters.'''\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.kaiming_normal(m.weight, mode='fan_out')\n",
    "            if m.bias:\n",
    "                init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.constant(m.weight, 1)\n",
    "            init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.normal(m.weight, std=1e-3)\n",
    "            if m.bias:\n",
    "                init.constant(m.bias, 0)\n",
    "\n",
    "\n",
    "try:\n",
    "\t_, term_width = os.popen('stty size', 'r').read().split()\n",
    "except:\n",
    "\tterm_width = 80\n",
    "term_width = int(term_width)\n",
    "\n",
    "TOTAL_BAR_LENGTH = 65.\n",
    "last_time = time.time()\n",
    "begin_time = last_time\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global last_time, begin_time\n",
    "    if current == 0:\n",
    "        begin_time = time.time()  # Reset for new bar.\n",
    "\n",
    "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(cur_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    cur_time = time.time()\n",
    "    step_time = cur_time - last_time\n",
    "    last_time = cur_time\n",
    "    tot_time = cur_time - begin_time\n",
    "\n",
    "    L = []\n",
    "    L.append('  Step: %s' % format_time(step_time))\n",
    "    L.append(' | Tot: %s' % format_time(tot_time))\n",
    "    if msg:\n",
    "        L.append(' | ' + msg)\n",
    "\n",
    "    msg = ''.join(L)\n",
    "    sys.stdout.write(msg)\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
    "        sys.stdout.write(' ')\n",
    "\n",
    "    # Go back to the center of the bar.\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
    "        sys.stdout.write('\\b')\n",
    "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
    "\n",
    "    if current < total-1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60\n",
    "    secondsf = int(seconds)\n",
    "    seconds = seconds - secondsf\n",
    "    millis = int(seconds*1000)\n",
    "\n",
    "    f = ''\n",
    "    i = 1\n",
    "    if days > 0:\n",
    "        f += str(days) + 'D'\n",
    "        i += 1\n",
    "    if hours > 0 and i <= 2:\n",
    "        f += str(hours) + 'h'\n",
    "        i += 1\n",
    "    if minutes > 0 and i <= 2:\n",
    "        f += str(minutes) + 'm'\n",
    "        i += 1\n",
    "    if secondsf > 0 and i <= 2:\n",
    "        f += str(secondsf) + 's'\n",
    "        i += 1\n",
    "    if millis > 0 and i <= 2:\n",
    "        f += str(millis) + 'ms'\n",
    "        i += 1\n",
    "    if f == '':\n",
    "        f = '0ms'\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0LJvRhYBxhFP"
   },
   "outputs": [],
   "source": [
    "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit_pytorch.py\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "MIN_NUM_PATCHES = 16\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias = False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # MASK HERE\n",
    "    def forward(self, x, mask = None):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
    "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
    "            mask = mask[:, None, :] * mask[:, :, None]\n",
    "            dots.masked_fill_(~mask, float('-inf'))\n",
    "            del mask\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, mlp_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Residual(PreNorm(dim, Attention(dim, heads = heads, dropout = dropout))),\n",
    "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)))\n",
    "            ]))\n",
    "    def forward(self, x, mask = None):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x, mask = mask)\n",
    "            x = ff(x)\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = channels * patch_size ** 2\n",
    "        assert num_patches > MIN_NUM_PATCHES, f'your number of patches ({num_patches}) is way too small for attention to be effective. try decreasing your patch size'\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, mlp_dim, dropout)\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, mask = None):\n",
    "        p = self.patch_size\n",
    "\n",
    "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.patch_to_embedding(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x, mask)\n",
    "\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rw6OuNQT0Ihc",
    "outputId": "f11726cc-97b5-454d-d54f-53fa306d63dd"
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "N8XGCiw4vPjc"
   },
   "outputs": [],
   "source": [
    "lr=1e-4 # learning rate. for resnets.. 1e-3, Vit..1e-4?\n",
    "opt=\"adam\"\n",
    "resume= 0  # resume from checkpoin\n",
    "aug=!      # add image augumentations\n",
    "mixup=1    # add mixup augumentations\n",
    "#net=vit\n",
    "bs=64\n",
    "n_epochs=100\n",
    "patch=4\n",
    "cos=0      # Train with cosine annealing scheduling\n",
    "\n",
    "if cos:\n",
    "    from warmup_scheduler import GradualWarmupScheduler\n",
    "if aug:\n",
    "    import albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119,
     "referenced_widgets": [
      "af26c51d6c404a9cb063f3ea64f57457",
      "3ce1264005014c44bd1d755f4d5d3d46",
      "cf0e1fff483142a49177cd37b53dad5c",
      "d6b5551314e54a808d71e124838a03ea",
      "1b8f2b2f0e00406987e19d1e7699a607",
      "491a0bbf2a694f2683961511459159a2",
      "9eda98a27a6347c5889943cbde318679",
      "82b0277c6c9d424da7983b24d8665657"
     ]
    },
    "id": "aiORS-0CzlEu",
    "outputId": "265919e9-99c6-48e6-d7e5-4f0453ec2cd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=8)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=8)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Original Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch0 = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = batch0[0], batch0[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = batch0[0][0], batch0[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         [-1.1497, -0.6650, -1.2660,  ..., -0.8977, -2.4291, -2.4291],\n",
       "         [-0.9752, -0.6069, -1.3241,  ..., -1.8087, -2.4291, -2.4291],\n",
       "         ...,\n",
       "         [-1.6731, -1.7118, -1.6924,  ..., -2.1964, -2.4291, -2.4291],\n",
       "         [-1.6731, -1.6149, -1.6343,  ..., -2.2546, -2.4291, -2.4291],\n",
       "         [-1.6537, -1.6149, -1.6149,  ..., -2.3321, -2.4291, -2.4291]],\n",
       "\n",
       "        [[-2.4183, -2.4183, -2.4183,  ..., -2.4183, -2.4183, -2.4183],\n",
       "         [-0.8646, -0.3532, -0.7662,  ..., -0.7859, -2.4183, -2.4183],\n",
       "         [-0.5892, -0.2549, -0.9236,  ..., -1.6512, -2.4183, -2.4183],\n",
       "         ...,\n",
       "         [-1.0612, -1.1792, -1.1989,  ..., -1.7496, -2.4183, -2.4183],\n",
       "         [-1.0416, -1.0416, -1.1399,  ..., -1.8676, -2.4183, -2.4183],\n",
       "         [-1.0612, -0.9629, -1.0416,  ..., -1.9659, -2.4183, -2.4183]],\n",
       "\n",
       "        [[-2.2214, -2.2214, -2.2214,  ..., -2.2214, -2.2214, -2.2214],\n",
       "         [-0.5045,  0.0028, -0.4655,  ..., -1.2654, -2.2214, -2.2214],\n",
       "         [-0.1533,  0.1394, -0.5240,  ..., -1.9092, -2.2214, -2.2214],\n",
       "         ...,\n",
       "         [-0.4069, -0.5045, -0.5630,  ..., -1.1678, -2.2214, -2.2214],\n",
       "         [-0.4264, -0.3874, -0.4850,  ..., -1.2654, -2.2214, -2.2214],\n",
       "         [-0.5240, -0.3679, -0.4264,  ..., -1.3825, -2.2214, -2.2214]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "patch_size = 4\n",
    "num_classes = 10\n",
    "channels = 3\n",
    "dim = 512\n",
    "depth = 6\n",
    "heads = 8\n",
    "mlp_dim = 512\n",
    "dropout_prob = 0.1\n",
    "emb_dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "patch_dim = channels * patch_size ** 2\n",
    "assert num_patches > MIN_NUM_PATCHES, f'your number of patches ({num_patches}) is way too small for attention to be effective. try decreasing your patch size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65, 512])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding.size() # 65 because we need to account for extra cls token to be prepended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_to_embedding = nn.Linear(patch_dim, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=48, out_features=512, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(patch_to_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token = nn.Parameter(torch.randn(1, 1, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(emb_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.1, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "print(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(dim, depth, heads, mlp_dim, dropout=dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (layers): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): ModuleList(\n",
      "      (0): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): ModuleList(\n",
      "      (0): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_cls_token = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identity()"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(mlp_dim, num_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (2): GELU()\n",
       "  (3): Dropout(p=0.1, inplace=False)\n",
       "  (4): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = patch_size\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 32, 32])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rearrange(images, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p) # get in form of (batch x num_patches x flattened_patch * num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 48])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(32*32*3)/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = patch_to_embedding(x) # pass through linear layer, proj 48 -> 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 512])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, n, _ = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 512)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, n, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_tokens = cls_token.expand(b, -1, -1) # broadcast cls token to length of one patch_embeddings, batch_size number of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 512])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_tokens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat((cls_tokens, x), dim=1) # prepend cls token to embedded image, KEEP IN MIND THIS IS DONE FOR EACH IMAGE IN THE BATCH (FIRST INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 65, 512])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65, 512])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding[:, :(n + 1)].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x += pos_embedding[:, :(n + 1)] # elementwise-add positional embedding to patch embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 65, 512])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 65, 512])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = transformer(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 65, 512])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identity()"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 512])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_cls_token(x[:, 0]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = to_cls_token(x[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 512])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (2): GELU()\n",
       "  (3): Dropout(p=0.1, inplace=False)\n",
       "  (4): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size() # project back to num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Super-Pixel Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch0 = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = batch0[0], batch0[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = batch0[0][0], batch0[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
       "         [-1.1497, -0.6650, -1.2660,  ..., -0.8977, -2.4291, -2.4291],\n",
       "         [-0.9752, -0.6069, -1.3241,  ..., -1.8087, -2.4291, -2.4291],\n",
       "         ...,\n",
       "         [-1.6731, -1.7118, -1.6924,  ..., -2.1964, -2.4291, -2.4291],\n",
       "         [-1.6731, -1.6149, -1.6343,  ..., -2.2546, -2.4291, -2.4291],\n",
       "         [-1.6537, -1.6149, -1.6149,  ..., -2.3321, -2.4291, -2.4291]],\n",
       "\n",
       "        [[-2.4183, -2.4183, -2.4183,  ..., -2.4183, -2.4183, -2.4183],\n",
       "         [-0.8646, -0.3532, -0.7662,  ..., -0.7859, -2.4183, -2.4183],\n",
       "         [-0.5892, -0.2549, -0.9236,  ..., -1.6512, -2.4183, -2.4183],\n",
       "         ...,\n",
       "         [-1.0612, -1.1792, -1.1989,  ..., -1.7496, -2.4183, -2.4183],\n",
       "         [-1.0416, -1.0416, -1.1399,  ..., -1.8676, -2.4183, -2.4183],\n",
       "         [-1.0612, -0.9629, -1.0416,  ..., -1.9659, -2.4183, -2.4183]],\n",
       "\n",
       "        [[-2.2214, -2.2214, -2.2214,  ..., -2.2214, -2.2214, -2.2214],\n",
       "         [-0.5045,  0.0028, -0.4655,  ..., -1.2654, -2.2214, -2.2214],\n",
       "         [-0.1533,  0.1394, -0.5240,  ..., -1.9092, -2.2214, -2.2214],\n",
       "         ...,\n",
       "         [-0.4069, -0.5045, -0.5630,  ..., -1.1678, -2.2214, -2.2214],\n",
       "         [-0.4264, -0.3874, -0.4850,  ..., -1.2654, -2.2214, -2.2214],\n",
       "         [-0.5240, -0.3679, -0.4264,  ..., -1.3825, -2.2214, -2.2214]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Superpixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import slic\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import img_as_float\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arr = image.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 32, 32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.numpy()[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arr = np.transpose(image_arr, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# apply SLIC and extract (approximately) the supplied number\n",
    "# of segments\n",
    "numSegments=50\n",
    "\n",
    "\n",
    "segments = slic(image_arr, \n",
    "                n_segments = numSegments, \n",
    "                sigma=3, \n",
    "                channel_axis=2,\n",
    "#                 slic_zero=True\n",
    "               )\n",
    "\n",
    "\n",
    "# show the output of SLIC\n",
    "fig = plt.figure(\"Superpixels -- %d segments\" % (numSegments), figsize=(5,5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.imshow(\n",
    "    mark_boundaries(image_arr, segments, mode='subpixel')\n",
    "         )\n",
    "\n",
    "plt.axis(\"off\")\n",
    "# show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(segments.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  2,  7,  7,  7],\n",
       "       [ 8,  7,  7,  7,  7],\n",
       "       [ 8,  7,  7,  7, 12],\n",
       "       [ 7,  7,  7, 12, 12],\n",
       "       [13, 13, 12, 12, 12]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments[5:10, 5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding dimension = channels * patch_size^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get the tensor in format of (batch_size x num_patches x flattened_patch * num_channels), then pass final index \n",
    "through linear layer Linear(idx2, embed_dim)\n",
    "\n",
    "embed_dim is 512 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "patch_size = 4\n",
    "num_classes = 10\n",
    "channels = 3\n",
    "dim = 512\n",
    "depth = 6\n",
    "heads = 8\n",
    "mlp_dim = 512\n",
    "dropout_prob = 0.1\n",
    "emb_dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "patch_dim = channels * patch_size ** 2\n",
    "assert num_patches > MIN_NUM_PATCHES, f'your number of patches ({num_patches}) is way too small for attention to be effective. try decreasing your patch size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65, 512])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding.size() # 65 because we need to account for extra cls token to be prepended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_to_embedding = nn.Linear(patch_dim, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=48, out_features=512, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(patch_to_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token = nn.Parameter(torch.randn(1, 1, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(emb_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.1, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "print(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(dim, depth, heads, mlp_dim, dropout=dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (layers): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): ModuleList(\n",
      "      (0): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): ModuleList(\n",
      "      (0): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Residual(\n",
      "        (fn): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_cls_token = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identity()"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(mlp_dim, num_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (2): GELU()\n",
       "  (3): Dropout(p=0.1, inplace=False)\n",
       "  (4): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = patch_size\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 32, 32])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rearrange(images, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 48])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(32*32*3)/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = patch_to_embedding(x) # pass through linear layer, proj 48 -> 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 512])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, n, _ = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 512)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, n, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_tokens = cls_token.expand(b, -1, -1) # broadcast cls token to length of one patch_embeddings, batch_size number of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 512])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_tokens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat((cls_tokens, x), dim=1) # prepend cls token to embedded image, KEEP IN MIND THIS IS DONE FOR EACH IMAGE IN THE BATCH (FIRST INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 65, 512])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65, 512])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding[:, :(n + 1)].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x += pos_embedding[:, :(n + 1)] # elementwise-add positional embedding to patch embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 65, 512])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 65, 512])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = transformer(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 65, 512])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identity()"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 512])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_cls_token(x[:, 0]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = to_cls_token(x[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 512])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (2): GELU()\n",
       "  (3): Dropout(p=0.1, inplace=False)\n",
       "  (4): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size() # project back to num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZLJUzJ2Y11nb"
   },
   "outputs": [],
   "source": [
    "# ViT for cifar10\n",
    "net = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = patch,\n",
    "    num_classes = 10,\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 512,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZw-kjtc2Pog",
    "outputId": "24977f64-1bb9-4cfa-f1e4-8016cbb29ab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;33mcheckpoint\u001b[0m/  jeo_vit_cifar10_imagenet.ipynb  jgl_vit_cifar10.ipynb  \u001b[38;5;33mlog\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ip3jcVzM18nw"
   },
   "outputs": [],
   "source": [
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net) # make parallel\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/{}-ckpt.t7'.format(\"vit\"))\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "# Loss is CE\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# reduce LR on Plateau\n",
    "if opt == \"adam\":\n",
    "    optimizer = optim.Adam(net.parameters(), lr)\n",
    "elif opt == \"sgd\":\n",
    "    optimizer = optim.SGD(net.parameters(), lr)    \n",
    "if not cos:\n",
    "    from torch.optim import lr_scheduler\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True, min_lr=1e-3*1e-5, factor=0.1)\n",
    "else:\n",
    "    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs-1)\n",
    "    scheduler = GradualWarmupScheduler(optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RVQqGdlH2mjh"
   },
   "outputs": [],
   "source": [
    "##### Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return train_loss/(batch_idx+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "08zVk9TxiTm_"
   },
   "outputs": [],
   "source": [
    "lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LE_ri1Wv2157",
    "outputId": "cdc20a1b-3ec8-4f81-aca3-290ac89b5ab8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " [================================================================>]  Step: 26ms | Tot: 39s158ms | Train Loss: 1.780 | Train Acc: 34.332% (17166/5000 782/782 02/782 ==============================>..................................]  Step: 51ms | Tot: 18s154ms | Train Loss: 1.950 | Train Acc: 27.050% (6267/2316 362/782 \n",
      " [================================================================>]  Step: 24ms | Tot: 3s322ms | Test Loss: 1.529 | Test Acc: 45.550% (4555/1000 100/100 \n",
      "Saving..\n",
      "Wed Mar 30 21:53:10 2022 Epoch 0, lr: 0.0001000, val loss: 152.87451, acc: 45.55000\n",
      "[152.87451028823853]\n",
      "\n",
      "Epoch: 1\n",
      " [==============================>..................................]  Step: 51ms | Tot: 18s467ms | Train Loss: 1.487 | Train Acc: 45.999% (10716/2329 364/782 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m list_acc \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, n_epochs):\n\u001b[0;32m---> 51\u001b[0m     trainloss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     val_loss, acc \u001b[38;5;241m=\u001b[39m test(epoch)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cos:\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m net(inputs)\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### Validation\n",
    "import time\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    \n",
    "    # Update scheduler\n",
    "    if not cos:\n",
    "        scheduler.step(test_loss)\n",
    "    \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/'+'vit'+'-{}-ckpt.t7'.format(patch))\n",
    "        best_acc = acc\n",
    "    \n",
    "    os.makedirs(\"log\", exist_ok=True)\n",
    "    content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, val loss: {test_loss:.5f}, acc: {(acc):.5f}'\n",
    "    print(content)\n",
    "    with open(f'log/log_vit.txt', 'a') as appender:\n",
    "        appender.write(content + \"\\n\")\n",
    "    return test_loss, acc\n",
    "\n",
    "list_loss = []\n",
    "list_acc = []\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    trainloss = train(epoch)\n",
    "    val_loss, acc = test(epoch)\n",
    "    \n",
    "    if cos:\n",
    "        scheduler.step(epoch-1)\n",
    "    \n",
    "    list_loss.append(val_loss)\n",
    "    list_acc.append(acc)\n",
    "    \n",
    "    # write as csv for analysis\n",
    "    with open(f'log/log_vit.csv', 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(list_loss) \n",
    "        writer.writerow(list_acc) \n",
    "    print(list_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPeKbkAmEjQU6is/SPJ+0Jq",
   "include_colab_link": true,
   "name": "Vision-Transformer-Cifar10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1b8f2b2f0e00406987e19d1e7699a607": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3ce1264005014c44bd1d755f4d5d3d46": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "491a0bbf2a694f2683961511459159a2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82b0277c6c9d424da7983b24d8665657": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9eda98a27a6347c5889943cbde318679": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af26c51d6c404a9cb063f3ea64f57457": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cf0e1fff483142a49177cd37b53dad5c",
       "IPY_MODEL_d6b5551314e54a808d71e124838a03ea"
      ],
      "layout": "IPY_MODEL_3ce1264005014c44bd1d755f4d5d3d46"
     }
    },
    "cf0e1fff483142a49177cd37b53dad5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_491a0bbf2a694f2683961511459159a2",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1b8f2b2f0e00406987e19d1e7699a607",
      "value": 1
     }
    },
    "d6b5551314e54a808d71e124838a03ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82b0277c6c9d424da7983b24d8665657",
      "placeholder": "​",
      "style": "IPY_MODEL_9eda98a27a6347c5889943cbde318679",
      "value": " 170500096/? [00:20&lt;00:00, 49468265.11it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
